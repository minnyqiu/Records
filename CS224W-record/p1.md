
https://github.com/TommyZihao/zihao_course/tree/main/CS224W

# p1: 斯坦福CS224W图机器学习、图神经网络、知识图谱【同济子豪兄】


## 1.1 图无处不在
Graphs are a general language for describing and analyzing entities with relations/interactions.
Graph: 节点和连接组成
自然界中，图是无处不在的这里的图，并不是平常所说的image图像，而是由节点和连接组成的graph，是用来描述关联数据的。关联数据的意思是数据样本之间是存在关系的，而图是用来描述这些关系的通用语言。在以往的机器学习中，数据往往没有关联，每一个数据样本都是彼此无关的、彼此独立的，即独立同分布，以mnist（m-nists）数据集为例，每一个手写数字图像之间都是没有联系的。我们用节点和连接把样本间的关系建立起来，变成一张图。像港铁路线图上，每一个站即一个节点，站与站之间的路线即连接。很多事物都可以抽象成图，以便于解决问题。

## 1.2 课程概述 
如何对带关联的图数据进行数据挖掘？
传统机器学习中，因为数据样本之间没有关联，分类任务需给出决策边界，回归问题则是拟合曲线。 对于图像、语音和文本，卷积神经网络用于处理图像（像素），循环神经网络和transformer用来处理语音或文字这类带序列的数据，图神经网络就是用来处理节点与连接这样图数据，有关联的数据的网络。
如何能够让神经网络处理带关联的数据？
不同于有尺寸的图像和有长度的文本，图是任意尺寸的，并且节点在输入网络时是没有顺序的，也没有参考锚点（比如卷积神经网络在对图像卷积时是按顺序，循环神经网络在处理文本时从前往后进行处理的）。并且图通常是动态变化的，并且有多模态的特征。譬如做音乐推荐，既有音频特征，又有歌词等关键词，以及听众的评论，或视频等。所以节点也是有多种模态的特征，如何利用这些特征，也是图神经网络特有的。
图神经网络的输入：图/网络，输出：节点的类别、节点间的新连接、新的图或新的子图。
中间的黑箱可以完成：端到端的表示学习，不需要人工提取特征，会自动学习特征，类似于卷积神经网络。表示学习：将节点表示为映射成d维的向量，向量包含了这个节点的连接关系，是一个包含了节点语义信息的嵌入向量。自动学习特征的过程就是将节点变成d维向量的过程。有了这个向量之后，可以用传统的机器学习或者深度学习的方法，再做分类或者拟合任务。
传统方法：Graphlets, Graph kernels
嵌入节点方法：DeepWalk, Node2Vec
图神经网络：GCN(图卷积), GraphSAGE, GAT(图注意力网络), Theory of GNNs
